{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "modelPruning",
      "provenance": [],
      "authorship_tag": "ABX9TyOpmHRPKAwEpTw2CQ659pfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gupta-keshav/model_pruning/blob/gupta-keshav-patch-1/modelPruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlkdFvEDMUf0"
      },
      "source": [
        "# Model Pruning\n",
        "Deep learning models trained these days are usually very bulky and inefficeint or if they are not bulky they tend to be somewhat inaccurate. This makes them unsuitable for deployment. It has been shown that it is possible to remove layers or neurons from the trained neural network without affecting the accruacy. The process of creating a more effiecient model from the given model is called model pruning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgff4e4XOty9"
      },
      "source": [
        "# importing the necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(48)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_LqctFnMNYj",
        "outputId": "e661bd96-7916-46d4-a88d-d600b5ea3cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# I will use the MNIST dataset for this task, loading it from the tensorflow itself\n",
        "dataset = tf.keras.datasets.mnist.load_data()\n",
        "X_train, y_train = dataset[0]\n",
        "X_test, y_test = dataset[1]\n",
        "X_train = np.expand_dims(X_train, axis=3)\n",
        "X_test = np.expand_dims(X_test, axis=3)\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VhDmpwjN876"
      },
      "source": [
        "Now I will build a toy CNN for the task "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iod_aBUEirtv"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3eQJoDHjQAJ"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu', name='conv_1'))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', name='conv_2'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', name='conv_3'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', name='conv_4'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.45))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmJocAHbxT7T",
        "outputId": "8294ae02-3816-4b3f-adae-57c0b7937527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "model.fit(X_train, to_categorical(y_train, 10), batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3608 - accuracy: 0.8934\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1050 - accuracy: 0.9717\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0783 - accuracy: 0.9777\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0653 - accuracy: 0.9818\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0574 - accuracy: 0.9841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efda8f0d208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgDrH-EQxrxO",
        "outputId": "8029a554-0987-4905-c80e-da16b5c42fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model.evaluate(X_test, to_categorical(y_test, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0305 - accuracy: 0.9917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.030478820204734802, 0.9916999936103821]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9KeBJ8NNOh4"
      },
      "source": [
        "# Technique 1\n",
        "In this technique we aim to remove the weights of neurons which are not necessary while making inference from the model, therefore making the model more efficient. We can also view this task as feature selecting i.e selecting only neurons that are necessary for model inference. Lasso regression which uses l1 norm in regression also is known for the feature selection, we will use l1 norm to determine the weights of neurons to be removed.\n",
        "\n",
        "Inspired by: lasso regression and \n",
        "Insipired by: https://github.com/Raukk/tf-keras-surgeon "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBlWsF-UNI-o",
        "outputId": "19acea99-1635-43f2-d3a9-194465287d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install kerassurgeon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kerassurgeon in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: pytest[test]<7.0.0,>=6.0.2 in /usr/local/lib/python3.6/dist-packages (from kerassurgeon) (6.1.1)\n",
            "\u001b[33m  WARNING: pytest 6.1.1 does not provide the extra 'test'\u001b[0m\n",
            "Requirement already satisfied: tensorflow<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from kerassurgeon) (2.3.0)\n",
            "Requirement already satisfied: pillow[examples]<8.0.0,>=7.2.0 in /usr/local/lib/python3.6/dist-packages (from kerassurgeon) (7.2.0)\n",
            "\u001b[33m  WARNING: Pillow 7.2.0 does not provide the extra 'examples'\u001b[0m\n",
            "Requirement already satisfied: keras[standalone-keras]<3.0.0,>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from kerassurgeon) (2.4.3)\n",
            "\u001b[33m  WARNING: Keras 2.4.3 does not provide the extra 'standalone-keras'\u001b[0m\n",
            "Requirement already satisfied: pandas[examples]<2.0.0,>=1.1.2 in /usr/local/lib/python3.6/dist-packages (from kerassurgeon) (1.1.2)\n",
            "\u001b[33m  WARNING: pandas 1.1.2 does not provide the extra 'examples'\u001b[0m\n",
            "Requirement already satisfied: importlib-metadata<2.0.0,>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from kerassurgeon) (1.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest[test]<7.0.0,>=6.0.2->kerassurgeon) (20.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest[test]<7.0.0,>=6.0.2->kerassurgeon) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest[test]<7.0.0,>=6.0.2->kerassurgeon) (20.2.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest[test]<7.0.0,>=6.0.2->kerassurgeon) (1.9.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest[test]<7.0.0,>=6.0.2->kerassurgeon) (0.13.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest[test]<7.0.0,>=6.0.2->kerassurgeon) (0.10.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (0.10.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (2.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (2.3.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3.0,>=2.0->kerassurgeon) (3.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras[standalone-keras]<3.0.0,>=2.4.3->kerassurgeon) (3.13)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas[examples]<2.0.0,>=1.1.2->kerassurgeon) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas[examples]<2.0.0,>=1.1.2->kerassurgeon) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata<2.0.0,>=1.7.0->kerassurgeon) (3.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest[test]<7.0.0,>=6.0.2->kerassurgeon) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow<3.0,>=2.0->kerassurgeon) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (1.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (0.2.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<3.0,>=2.0->kerassurgeon) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qtjBFsMFPTX"
      },
      "source": [
        "from kerassurgeon.operations import delete_channels, Surgeon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmm0Vkf9z92L",
        "outputId": "4c98049b-7f42-46dc-9df4-832afa128d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "weights_conv = model.get_layer('conv_3').get_weights()[0] #getting the weights of the layer\n",
        "\n",
        "weights_dict = {}\n",
        "num_filters = len(weights_conv[0, 0, 0, :])\n",
        "for j in range(num_filters):\n",
        "    w_s = np.sum(abs(weights_conv[:, :, :, j])) # l1_norm of the channel j\n",
        "    filt = f'filt_{j}'\n",
        "    weights_dict[filt] = w_s \n",
        "\n",
        "weights_dict_sort = sorted(weights_dict.items(), key=lambda kv: kv[1]) #dictionary containing the filter number and its l1_norm sorted in ascending order according to the norm\n",
        "print(weights_dict_sort)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('filt_45', 14.014978), ('filt_63', 14.89159), ('filt_13', 15.573659), ('filt_26', 16.545626), ('filt_44', 16.613613), ('filt_29', 16.743008), ('filt_36', 16.980133), ('filt_21', 17.157934), ('filt_10', 17.32591), ('filt_2', 17.380947), ('filt_31', 17.457314), ('filt_60', 17.461067), ('filt_42', 17.481697), ('filt_59', 17.597744), ('filt_8', 17.646385), ('filt_38', 17.737852), ('filt_3', 17.770887), ('filt_40', 17.794525), ('filt_56', 17.818424), ('filt_43', 17.832178), ('filt_57', 17.895323), ('filt_0', 17.940216), ('filt_39', 17.944183), ('filt_6', 18.028564), ('filt_5', 18.200184), ('filt_55', 18.323029), ('filt_32', 18.44989), ('filt_41', 18.475973), ('filt_35', 18.50659), ('filt_23', 18.597221), ('filt_62', 18.95551), ('filt_61', 19.210464), ('filt_49', 19.350111), ('filt_28', 19.35201), ('filt_53', 19.421154), ('filt_18', 19.433388), ('filt_9', 19.44281), ('filt_12', 19.479355), ('filt_11', 19.533556), ('filt_51', 19.540794), ('filt_34', 19.680637), ('filt_22', 19.709835), ('filt_24', 19.730194), ('filt_33', 19.78554), ('filt_48', 19.884338), ('filt_27', 19.921196), ('filt_17', 20.277449), ('filt_4', 20.374413), ('filt_58', 20.52158), ('filt_16', 20.637709), ('filt_14', 20.73827), ('filt_15', 20.906204), ('filt_7', 20.96971), ('filt_54', 21.030125), ('filt_46', 21.227554), ('filt_52', 21.241076), ('filt_47', 21.37469), ('filt_20', 21.420858), ('filt_1', 21.790348), ('filt_50', 21.797104), ('filt_19', 22.146355), ('filt_30', 22.594925), ('filt_25', 22.736652), ('filt_37', 23.789417)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpEBIY-h9_hd",
        "outputId": "2b22889d-ceb8-40fc-9656-e0f2724318a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "num_channels = 8 #number of channels to be deleted\n",
        "layer_3 = model.get_layer('conv_3') #layer from which the channels are to be deleted\n",
        "channels_3 = [int(weights_dict_sort[i][0].split('_')[1]) for i in range(num_channels)]\n",
        "model_new = delete_channels(model, layer_3, channels_3)\n",
        "model_new.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_new.evaluate(X_test, to_categorical(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting 8/64 channels from layer: conv_3\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0312 - accuracy: 0.9918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.031220389530062675, 0.9918000102043152]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoa8HGJF-qYX",
        "outputId": "42480635-1ba3-4520-9d8f-1beaf8cbf0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "weights_conv = model.get_layer('conv_4').get_weights()[0]\n",
        "\n",
        "weights_dict = {}\n",
        "num_filters = len(weights_conv[0, 0, 0, :])\n",
        "for j in range(num_filters):\n",
        "    w_s = np.sum(abs(weights_conv[:, :, :, j]))\n",
        "    filt = f'filt_{j}'\n",
        "    weights_dict[filt] = w_s\n",
        "\n",
        "weights_dict_sort = sorted(weights_dict.items(), key=lambda kv: kv[1])\n",
        "print(weights_dict_sort)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('filt_23', 25.708128), ('filt_12', 27.253494), ('filt_16', 27.38153), ('filt_50', 27.714851), ('filt_35', 28.69423), ('filt_59', 29.956463), ('filt_27', 30.074177), ('filt_32', 30.64943), ('filt_60', 31.05884), ('filt_26', 31.355495), ('filt_18', 31.574871), ('filt_57', 31.81504), ('filt_25', 32.74515), ('filt_19', 32.971474), ('filt_58', 32.972313), ('filt_53', 33.080093), ('filt_45', 33.71131), ('filt_1', 34.007626), ('filt_7', 34.073246), ('filt_15', 34.34056), ('filt_52', 34.471264), ('filt_39', 34.512405), ('filt_40', 34.560204), ('filt_37', 34.85219), ('filt_38', 34.917923), ('filt_2', 35.015335), ('filt_48', 35.4364), ('filt_56', 35.659134), ('filt_33', 35.88742), ('filt_13', 36.248505), ('filt_28', 36.353096), ('filt_49', 36.457817), ('filt_6', 36.609264), ('filt_17', 36.625816), ('filt_46', 36.650696), ('filt_34', 36.74626), ('filt_42', 36.895634), ('filt_51', 37.035347), ('filt_24', 37.155174), ('filt_14', 37.305454), ('filt_43', 37.840706), ('filt_31', 38.12336), ('filt_21', 38.282295), ('filt_22', 38.42266), ('filt_54', 38.66387), ('filt_47', 38.723526), ('filt_63', 38.748825), ('filt_62', 38.946026), ('filt_29', 39.173187), ('filt_5', 39.188366), ('filt_30', 39.234703), ('filt_3', 39.389687), ('filt_55', 39.53454), ('filt_10', 39.651176), ('filt_36', 40.321747), ('filt_9', 40.50467), ('filt_61', 40.602936), ('filt_11', 40.952156), ('filt_0', 41.506042), ('filt_44', 41.537163), ('filt_20', 41.570633), ('filt_41', 42.013557), ('filt_8', 42.221172), ('filt_4', 43.305473)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMVg3-VPH4U6",
        "outputId": "3ce27b50-e948-470e-95b7-b0a5a19f7047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "num_channels = 6\n",
        "layer_4 = model.get_layer('conv_4')\n",
        "channels_4 = [int(weights_dict_sort[i][0].split('_')[1]) for i in range(num_channels)]\n",
        "model_new = delete_channels(model, layer_4, channels_4)\n",
        "model_new.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_new.evaluate(X_test, to_categorical(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting 6/64 channels from layer: conv_4\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0302 - accuracy: 0.9916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.030247937887907028, 0.991599977016449]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpyIsdX_H5wy",
        "outputId": "82653411-3055-4f5e-f1e2-d5f7d450ad34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "'''\n",
        " Instead of deleting from just a single layer, now I will delete channel from multiple layers.\n",
        "'''\n",
        "surgeon = Surgeon(model)\n",
        "surgeon.add_job('delete_channels', layer_3, channels=channels_3)\n",
        "surgeon.add_job('delete_channels', layer_4, channels=channels_4)\n",
        "model_new = surgeon.operate()\n",
        "model_new.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_new.evaluate(X_test, to_categorical(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting 8/64 channels from layer: conv_3\n",
            "Deleting 6/64 channels from layer: conv_4\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0310 - accuracy: 0.9918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.031038757413625717, 0.9918000102043152]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zal8lr9jNRyx"
      },
      "source": [
        "# Technique 2\n",
        "### using tensorflow method insipired by the paper \"To prune, or not to prune: exploring the efficacy of pruning for model compression\"\n",
        "blog: https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEg51xskoU-0"
      },
      "source": [
        "pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl1OewQNoisP"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "import tempfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weKzffKwom2u",
        "outputId": "e203016a-d767-40cf-ddb2-d253dce5b19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model)\n",
        "model_for_pruning.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:200: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:200: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv_1 ( (None, 26, 26, 32)        610       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv_2 ( (None, 24, 24, 32)        18466     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 12, 12, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv_3 ( (None, 10, 10, 64)        36930     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv_4 ( (None, 8, 8, 64)          73794     \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 4, 4, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 1024)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 64)                131138    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout  (None, 64)                1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 10)                1292      \n",
            "=================================================================\n",
            "Total params: 262,234\n",
            "Trainable params: 131,242\n",
            "Non-trainable params: 130,992\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPsuyZQ_owL6"
      },
      "source": [
        "# log_dir = tempfile.mkdtemp()\n",
        "callbacks = [\n",
        "             tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "            #  tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT7h5x5Npd7f",
        "outputId": "acfa4b5e-a6b4-4753-f91d-66bf18eda951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "model_for_pruning.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_for_pruning.fit(X_train, to_categorical(y_train, 10), batch_size=32, callbacks=callbacks, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0494 - accuracy: 0.9854\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0365 - accuracy: 0.9890\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0307 - accuracy: 0.9909\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0278 - accuracy: 0.9915\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0251 - accuracy: 0.9928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd7f714b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfXUD_SFqAfB",
        "outputId": "1c5cd55a-e2f1-46a3-f72c-6ba8cd36141f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model_for_pruning.evaluate(X_test, to_categorical(y_test, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0266 - accuracy: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.026550406590104103, 0.9940000176429749]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zyJhv7wqmFQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}